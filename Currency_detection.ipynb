{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesjoseph/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not read /Users/jamesjoseph/Jamezzz/Final project/IntoTheProject/Indian_currency_dataset/training/10/.DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not read /Users/jamesjoseph/Jamezzz/Final project/IntoTheProject/Indian_currency_dataset/training/20/.DS_Store\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n",
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "TRAIN_DIR = \"/Users/jamesjoseph/Jamezzz/Final project/IntoTheProject/Indian_currency_dataset/training\"\n",
    "\n",
    "IMAGE_SIZE = (224, 224)  \n",
    "CLASSES = [\"10\", \"20\", \"50\", \"100\", \"200\", \"500\"]  # Denominations\n",
    "\n",
    "\n",
    "def load_images_from_directory(directory):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for label in CLASSES:\n",
    "        folder_path = os.path.join(directory, label)  \n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"Warning: Folder for denomination {label} not found!\")\n",
    "            continue\n",
    "        \n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            if img is None:\n",
    "                print(f\"Warning: Could not read {img_path}\")\n",
    "                continue\n",
    "\n",
    "            img = cv2.resize(img, IMAGE_SIZE)\n",
    "            images.append(img)\n",
    "            labels.append(CLASSES.index(label)) \n",
    "\n",
    "    images = np.array(images) / 255.0\n",
    "    labels = to_categorical(labels, num_classes=len(CLASSES))  # One-hot encoding\n",
    "\n",
    "    return images, labels\n",
    "images,labels=load_images_from_directory(TRAIN_DIR)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "# Load pre-trained model\n",
    "base_model = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "base_model.trainable = False\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(len(CLASSES), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 411ms/step - accuracy: 0.2610 - loss: 4.6035 - val_accuracy: 0.5009 - val_loss: 1.2619\n",
      "Epoch 2/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 349ms/step - accuracy: 0.5258 - loss: 1.2352 - val_accuracy: 0.6216 - val_loss: 1.0220\n",
      "Epoch 3/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 345ms/step - accuracy: 0.6186 - loss: 1.0074 - val_accuracy: 0.6252 - val_loss: 1.0595\n",
      "Epoch 4/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 355ms/step - accuracy: 0.6349 - loss: 0.9707 - val_accuracy: 0.6453 - val_loss: 0.9881\n",
      "Epoch 5/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 351ms/step - accuracy: 0.7194 - loss: 0.7845 - val_accuracy: 0.6837 - val_loss: 0.8846\n",
      "Epoch 6/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 357ms/step - accuracy: 0.7323 - loss: 0.7503 - val_accuracy: 0.7130 - val_loss: 0.8254\n",
      "Epoch 7/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 352ms/step - accuracy: 0.7361 - loss: 0.7252 - val_accuracy: 0.7477 - val_loss: 0.7362\n",
      "Epoch 8/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 353ms/step - accuracy: 0.7507 - loss: 0.6940 - val_accuracy: 0.7294 - val_loss: 0.7500\n",
      "Epoch 9/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 362ms/step - accuracy: 0.7710 - loss: 0.6420 - val_accuracy: 0.7203 - val_loss: 0.8065\n",
      "Epoch 10/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 361ms/step - accuracy: 0.7745 - loss: 0.6221 - val_accuracy: 0.7148 - val_loss: 0.8543\n",
      "Epoch 11/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 360ms/step - accuracy: 0.7637 - loss: 0.6903 - val_accuracy: 0.7057 - val_loss: 0.8491\n",
      "Epoch 12/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 361ms/step - accuracy: 0.7581 - loss: 0.6445 - val_accuracy: 0.7221 - val_loss: 0.8062\n",
      "Epoch 13/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 357ms/step - accuracy: 0.8103 - loss: 0.5530 - val_accuracy: 0.7623 - val_loss: 0.7606\n",
      "Epoch 14/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 371ms/step - accuracy: 0.7954 - loss: 0.5685 - val_accuracy: 0.7075 - val_loss: 0.8677\n",
      "Epoch 15/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 363ms/step - accuracy: 0.7953 - loss: 0.5926 - val_accuracy: 0.7057 - val_loss: 0.8173\n",
      "Epoch 16/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 363ms/step - accuracy: 0.7934 - loss: 0.5369 - val_accuracy: 0.7477 - val_loss: 0.7717\n",
      "Epoch 17/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 368ms/step - accuracy: 0.8197 - loss: 0.5040 - val_accuracy: 0.7294 - val_loss: 0.8066\n",
      "Epoch 18/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 374ms/step - accuracy: 0.8047 - loss: 0.5431 - val_accuracy: 0.7751 - val_loss: 0.7279\n",
      "Epoch 19/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 359ms/step - accuracy: 0.8266 - loss: 0.5103 - val_accuracy: 0.7550 - val_loss: 0.7272\n",
      "Epoch 20/20\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 370ms/step - accuracy: 0.8346 - loss: 0.4759 - val_accuracy: 0.7806 - val_loss: 0.7671\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 241ms/step - accuracy: 0.7674 - loss: 0.7969\n",
      "Test Accuracy: 78.06%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"currency_detection_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
