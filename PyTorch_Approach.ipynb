{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f835c8-682d-4ce9-92de-cb7c8a001161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc67227-48ce-4ee8-a85f-8757454d9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "def pad_or_trim(mel_spec, max_frames=300):\n",
    "    \"\"\"\n",
    "    mel_spec: [1, n_mels, time]\n",
    "    max_frames: desired fixed time dimension\n",
    "    \"\"\"\n",
    "    channels, n_mels, time = mel_spec.shape\n",
    "    if time < max_frames:\n",
    "        # Pad with zeros at the end\n",
    "        pad_size = max_frames - time\n",
    "        pad = torch.zeros((channels, n_mels, pad_size), device=mel_spec.device)\n",
    "        mel_spec = torch.cat([mel_spec, pad], dim=2)\n",
    "    elif time > max_frames:\n",
    "        # Random crop\n",
    "        start = random.randint(0, time - max_frames)\n",
    "        mel_spec = mel_spec[:, :, start:start+max_frames]\n",
    "    return mel_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6d1964-1f39-4b4a-b8e6-37e62fa75af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import soundfile as sf\n",
    "\n",
    "class TripletSpeakerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creates triplets (anchor, positive, negative) for metric learning.\n",
    "    - anchor, positive: same speaker\n",
    "    - negative: different speaker\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, n_mels=40, transform=None):\n",
    "        \"\"\"\n",
    "        data_root: root folder, subfolders named '1','2',... each with .wav chunks\n",
    "        n_mels: number of mel filter banks\n",
    "        transform: optional transform on the mel-spectrogram\n",
    "        \"\"\"\n",
    "        self.data_root = data_root\n",
    "        self.n_mels = n_mels\n",
    "        self.transform = transform\n",
    "\n",
    "        # Gather speaker folders\n",
    "        self.speakers = sorted([\n",
    "            d for d in os.listdir(data_root)\n",
    "            if os.path.isdir(os.path.join(data_root, d))\n",
    "        ])\n",
    "        # speaker -> list of .wav file paths\n",
    "        self.speaker_files = {}\n",
    "        for spk in self.speakers:\n",
    "            spk_dir = os.path.join(data_root, spk)\n",
    "            wavs = [os.path.join(spk_dir, f)\n",
    "                    for f in os.listdir(spk_dir) \n",
    "                    if f.lower().endswith('.wav')]\n",
    "            self.speaker_files[spk] = wavs\n",
    "\n",
    "        # Flatten all (speaker, file) pairs to create an indexable list\n",
    "        self.index_list = []\n",
    "        for spk in self.speakers:\n",
    "            for wavpath in self.speaker_files[spk]:\n",
    "                self.index_list.append((spk, wavpath))\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=16000,  # adjust if needed\n",
    "            n_mels=self.n_mels\n",
    "        )\n",
    "        self.to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) Get anchor info\n",
    "        anchor_spk, anchor_path = self.index_list[idx]\n",
    "\n",
    "        # 2) Randomly pick a positive sample from the same speaker\n",
    "        positive_path = random.choice(self.speaker_files[anchor_spk])\n",
    "        while positive_path == anchor_path and len(self.speaker_files[anchor_spk]) > 1:\n",
    "            # ensure we pick a different file if possible\n",
    "            positive_path = random.choice(self.speaker_files[anchor_spk])\n",
    "\n",
    "        # 3) Pick a negative speaker\n",
    "        neg_spk = random.choice(self.speakers)\n",
    "        while neg_spk == anchor_spk and len(self.speakers) > 1:\n",
    "            neg_spk = random.choice(self.speakers)\n",
    "        negative_path = random.choice(self.speaker_files[neg_spk])\n",
    "\n",
    "        # Load & transform the audio for anchor, positive, negative\n",
    "        anchor_mel = self._wav_to_mel(anchor_path)\n",
    "        positive_mel = self._wav_to_mel(positive_path)\n",
    "        negative_mel = self._wav_to_mel(negative_path)\n",
    "\n",
    "        return anchor_mel, positive_mel, negative_mel\n",
    "\n",
    "    def _wav_to_mel(self, file_path):\n",
    "        audio_data, sr = sf.read(file_path)\n",
    "        audio_tensor = torch.from_numpy(audio_data).float()\n",
    "        if audio_tensor.dim() == 1:\n",
    "            audio_tensor = audio_tensor.unsqueeze(0)  # [1, samples]\n",
    "        if sr != 16000:\n",
    "            audio_tensor = torchaudio.functional.resample(audio_tensor, sr, 16000)\n",
    "            sr = 16000\n",
    "\n",
    "        # Mel spectrogram\n",
    "        mel_spec = self.mel_transform(audio_tensor)\n",
    "        mel_spec = self.to_db(mel_spec)\n",
    "        mel_spec = pad_or_trim(mel_spec, max_frames=300)\n",
    "        # shape: [channel=1, n_mels, time]\n",
    "        if self.transform:\n",
    "            mel_spec = self.transform(mel_spec)\n",
    "\n",
    "        return mel_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cb0889b-7c49-4bb7-8674-876e70a32b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SpeakerEmbeddingNet(nn.Module):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc = nn.Linear(64, embed_dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.global_pool(x) \n",
    "        x = x.view(x.size(0), -1)  # [B, 64]\n",
    "        x = self.fc(x)            # [B, embed_dim]\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3c8aa0-5972-48a3-914b-73a11e0191c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def triplet_loss(anchor_emb, positive_emb, negative_emb, margin=1.0):\n",
    "    \"\"\"\n",
    "    Computes the triplet loss using cosine similarity.\n",
    "    anchor_emb, positive_emb, negative_emb: [B, embed_dim]\n",
    "    margin: Margin for the triplet loss.\n",
    "    \"\"\"\n",
    "    # Cosine similarity\n",
    "    cos_sim_pos = F.cosine_similarity(anchor_emb, positive_emb)  # [B]\n",
    "    cos_sim_neg = F.cosine_similarity(anchor_emb, negative_emb)  # [B]\n",
    "\n",
    "    # Convert cosine similarity to distance\n",
    "    dist_pos = 1 - cos_sim_pos  # [B]\n",
    "    dist_neg = 1 - cos_sim_neg  # [B]\n",
    "\n",
    "    # Compute the triplet loss\n",
    "    losses = torch.relu(dist_pos - dist_neg + margin)  # [B]\n",
    "    return losses.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa977e9c-bc01-4224-8f63-6bac374a7ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "def train_siamese(train_loader, val_loader, embed_dim=128, epochs=10, lr=1e-3, margin=1.0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SpeakerEmbeddingNet(embed_dim=embed_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, total_batches = 0.0, 0\n",
    "        for anchor_mel, pos_mel, neg_mel in train_loader:\n",
    "            anchor_mel = anchor_mel.to(device)\n",
    "            pos_mel = pos_mel.to(device)\n",
    "            neg_mel = neg_mel.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            anchor_emb = model(anchor_mel)\n",
    "            pos_emb = model(pos_mel)\n",
    "            neg_emb = model(neg_mel)\n",
    "\n",
    "            loss = triplet_loss(anchor_emb, pos_emb, neg_emb, margin=margin)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "        avg_train_loss = total_loss / total_batches\n",
    "\n",
    "        # Evaluate on val_loader\n",
    "        model.eval()\n",
    "        val_loss, val_batches = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for anchor_mel, pos_mel, neg_mel in val_loader:\n",
    "                anchor_mel = anchor_mel.to(device)\n",
    "                pos_mel = pos_mel.to(device)\n",
    "                neg_mel = neg_mel.to(device)\n",
    "\n",
    "                anchor_emb = model(anchor_mel)\n",
    "                pos_emb = model(pos_mel)\n",
    "                neg_emb = model(neg_mel)\n",
    "\n",
    "                loss = triplet_loss(anchor_emb, pos_emb, neg_emb, margin=margin)\n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "\n",
    "        avg_val_loss = val_loss / val_batches if val_batches > 0 else 0\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {avg_train_loss:.4f} Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6179f9be-bec4-4ce2-af28-c97d3af264a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTUS\\AppData\\Local\\Temp\\ipykernel_6724\\1771427983.py:3: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")      # Use \"soundfile\" backend if needed on Windows\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Train Loss: 0.7589 Val Loss: 0.6072\n",
      "Epoch [2/10] Train Loss: 0.5776 Val Loss: 0.6352\n",
      "Epoch [3/10] Train Loss: 0.5115 Val Loss: 0.4986\n",
      "Epoch [4/10] Train Loss: 0.4650 Val Loss: 0.4889\n",
      "Epoch [5/10] Train Loss: 0.4571 Val Loss: 0.4589\n",
      "Epoch [6/10] Train Loss: 0.4467 Val Loss: 0.4499\n",
      "Epoch [7/10] Train Loss: 0.4393 Val Loss: 0.4915\n",
      "Epoch [8/10] Train Loss: 0.4203 Val Loss: 0.4454\n",
      "Epoch [9/10] Train Loss: 0.4212 Val Loss: 0.4796\n",
      "Epoch [10/10] Train Loss: 0.4385 Val Loss: 0.4433\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"soundfile\")      # Use \"soundfile\" backend if needed on Windows\n",
    "\n",
    "data_root = r\"C:\\Users\\VICTUS\\Voice\\A Dataset for Voice-Based Human Identity Recognition\\output\"\n",
    "full_dataset = TripletSpeakerDataset(data_root)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "model = train_siamese(train_loader, val_loader, embed_dim=128, epochs=10, lr=1e-3, margin=1.0)\n",
    "torch.save(model.state_dict(), \"siamese_speaker_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c31dd173-ad72-42da-830e-7451dd5ec6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def enroll_speaker(model, file_paths, device='cpu'):\n",
    "    \"\"\"\n",
    "    file_paths: list of .wav files for this speaker's enrollment\n",
    "    returns: an average embedding (torch.Tensor) to represent the speaker\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    for fp in file_paths:\n",
    "        mel_spec = load_mel_spec(fp)  # same transform used in dataset\n",
    "        mel_spec = mel_spec.unsqueeze(0).to(device)  # [1, 1, n_mels, time]\n",
    "        with torch.no_grad():\n",
    "            emb = model(mel_spec)\n",
    "        embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "    # average\n",
    "    avg_emb = np.mean(embeddings, axis=0)  # shape [1, embed_dim]\n",
    "    avg_emb = torch.from_numpy(avg_emb).float()\n",
    "    avg_emb = torch.nn.functional.normalize(avg_emb, p=2, dim=1)\n",
    "    return avg_emb\n",
    "\n",
    "def load_mel_spec(file_path, sr=16000, n_mels=40):\n",
    "    \"\"\"\n",
    "    Quick utility to load a .wav and return a Mel-spectrogram (1, n_mels, time).\n",
    "    Must match the approach used in your dataset's _wav_to_mel or transforms.\n",
    "    \"\"\"\n",
    "    audio_data, orig_sr = sf.read(file_path)\n",
    "    audio_tensor = torch.from_numpy(audio_data).float().unsqueeze(0)\n",
    "    if orig_sr != sr:\n",
    "        audio_tensor = torchaudio.functional.resample(audio_tensor, orig_sr, sr)\n",
    "\n",
    "    mel_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_mels=n_mels)\n",
    "    to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "    mel_spec = to_db(mel_transform(audio_tensor))\n",
    "    return mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6bd2e7f-4de8-4036-89c6-a94080cafeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_speaker(model, enrolled_embedding, test_wav, threshold=0.5, device='cpu'):\n",
    "    \"\"\"\n",
    "    Compare test_wav embedding to the enrolled_embedding (already stored).\n",
    "    Return 'ACCEPT' if distance < threshold, else 'REJECT'.\n",
    "\n",
    "    threshold is a distance threshold in embedding space.\n",
    "    If you L2-normalize embeddings, typical threshold might be around 0.5~1.0\n",
    "    (Tune on a dev set).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    mel_spec = load_mel_spec(test_wav).to(device)\n",
    "    mel_spec = mel_spec.unsqueeze(0)  # [1, 1, n_mels, time]\n",
    "    with torch.no_grad():\n",
    "        test_emb = model(mel_spec)     # [1, embed_dim]\n",
    "    # L2-normalize\n",
    "    test_emb = F.normalize(test_emb, p=2, dim=1)\n",
    "\n",
    "    # distance\n",
    "    dist = torch.norm(test_emb - enrolled_embedding.to(device), p=2).item()\n",
    "    print(f\"Distance to enrolled embedding: {dist:.3f}\")\n",
    "    if dist < threshold:\n",
    "        return \"ACCEPT\"\n",
    "    else:\n",
    "        return \"REJECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c2dbbf7-4fed-4e88-ba87-75d6d1cc07de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTUS\\AppData\\Local\\Temp\\ipykernel_6724\\2239093558.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"siamese_speaker_model.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Accuracy: 80.10% over 1000 pairs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import os\n",
    "import soundfile as sf\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "##################################\n",
    "# 1. Create a Pairwise Test Dataset\n",
    "##################################\n",
    "class VerificationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Yields (mel1, mel2, label), where label=1 if same speaker, 0 otherwise.\n",
    "    We assume we have a root folder with subfolders for each speaker.\n",
    "    We'll sample pairs from the same speaker for label=1, and from different speakers for label=0.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_root, n_mels=40, num_pairs=2000):\n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.n_mels = n_mels\n",
    "        self.speakers = sorted([\n",
    "            d for d in os.listdir(data_root)\n",
    "            if os.path.isdir(os.path.join(data_root, d))\n",
    "        ])\n",
    "        self.speaker_files = {}\n",
    "        for spk in self.speakers:\n",
    "            spk_dir = os.path.join(data_root, spk)\n",
    "            wavs = [\n",
    "                os.path.join(spk_dir, f)\n",
    "                for f in os.listdir(spk_dir)\n",
    "                if f.lower().endswith('.wav')\n",
    "            ]\n",
    "            self.speaker_files[spk] = wavs\n",
    "\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=16000,  # adjust to your sr\n",
    "            n_mels=self.n_mels\n",
    "        )\n",
    "        self.to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "        self.pairs = []\n",
    "        \n",
    "        half_pairs = num_pairs // 2     # half same-speaker, half different\n",
    "\n",
    "        # same-speaker pairs\n",
    "        for _ in range(half_pairs):\n",
    "            spk = random.choice(self.speakers)\n",
    "            if len(self.speaker_files[spk]) < 2:\n",
    "                continue\n",
    "            path1 = random.choice(self.speaker_files[spk])\n",
    "            path2 = random.choice(self.speaker_files[spk])\n",
    "            self.pairs.append((path1, path2, 1))\n",
    "\n",
    "        # different-speaker pairs\n",
    "        for _ in range(half_pairs):\n",
    "            spk1, spk2 = random.sample(self.speakers, 2)\n",
    "            path1 = random.choice(self.speaker_files[spk1])\n",
    "            path2 = random.choice(self.speaker_files[spk2])\n",
    "            self.pairs.append((path1, path2, 0))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path1, path2, label = self.pairs[idx]\n",
    "        mel1 = self._wav_to_mel(path1)\n",
    "        mel2 = self._wav_to_mel(path2)\n",
    "        return mel1, mel2, label\n",
    "\n",
    "    def _wav_to_mel(self, file_path):\n",
    "        audio_data, sr = sf.read(file_path)\n",
    "        audio_tensor = torch.from_numpy(audio_data).float().unsqueeze(0)\n",
    "        if sr != 16000:\n",
    "            audio_tensor = torchaudio.functional.resample(audio_tensor, sr, 16000)\n",
    "\n",
    "        mel_spec = self.mel_transform(audio_tensor)\n",
    "        mel_spec = self.to_db(mel_spec)\n",
    "        mel_spec = pad_or_trim(mel_spec, max_frames=300)\n",
    "        return mel_spec\n",
    "\n",
    "##################################\n",
    "# 2. Evaluate the Model's Verification Accuracy\n",
    "##################################\n",
    "def evaluate_verification(model, loader, threshold=0.8, device='cpu'):\n",
    "    \"\"\"\n",
    "    model: your Siamese or embedding network returning [batch, embed_dim].\n",
    "    loader: yields pairs of mel-spectrogram + label.\n",
    "    threshold: distance threshold below which we predict 'same speaker'.\n",
    "    Returns: (accuracy, total_samples)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for mel1, mel2, label in loader:\n",
    "            mel1 = mel1.to(device)\n",
    "            mel2 = mel2.to(device)\n",
    "\n",
    "            emb1 = model(mel1)  # [B, embed_dim]\n",
    "            emb2 = model(mel2)  # [B, embed_dim]\n",
    "\n",
    "            # L2 normalize if your model doesn't already\n",
    "            emb1 = F.normalize(emb1, p=2, dim=1)\n",
    "            emb2 = F.normalize(emb2, p=2, dim=1)\n",
    "\n",
    "            # Euclidean distance\n",
    "            dist = torch.norm(emb1 - emb2, p=2, dim=1)\n",
    "\n",
    "            # Predict same-speaker if dist < threshold\n",
    "            pred_same = (dist < threshold).long()\n",
    "            label = label.to(device).long()\n",
    "\n",
    "            correct += (pred_same == label).sum().item()\n",
    "            total += label.size(0)\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    return accuracy, total\n",
    "\n",
    "\n",
    "##################################\n",
    "# 3. Usage Example (After Training)\n",
    "##################################\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = SpeakerEmbeddingNet(embed_dim=128).to(device)\n",
    "    model.load_state_dict(torch.load(\"siamese_speaker_model.pth\", map_location=device))\n",
    "\n",
    "    data_root = r\"C:\\Users\\VICTUS\\Voice\\A Dataset for Voice-Based Human Identity Recognition\\output\"\n",
    "    verification_dataset = VerificationDataset(data_root, n_mels=40, num_pairs=1000)\n",
    "    verif_loader = DataLoader(verification_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    threshold = 0.8 \n",
    "    acc, total = evaluate_verification(model, verif_loader, threshold=threshold, device=device)\n",
    "    print(f\"Verification Accuracy: {acc*100:.2f}% over {total} pairs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4dc8e-50aa-4f01-a8e0-26a52eba2a72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3d1aa-f963-45e6-a842-b4adda36d8a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
